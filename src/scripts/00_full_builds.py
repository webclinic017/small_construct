# -*- coding: utf-8 -*-
"""00_Full_Builds.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10I7Cz5n2lfQlRFNGQqCUnWo31TLtcsUw

<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#CANDLE_STICK" data-toc-modified-id="CANDLE_STICK-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>CANDLE_STICK</a></span></li><li><span><a href="#Compare_Returns" data-toc-modified-id="Compare_Returns-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Compare_Returns</a></span></li><li><span><a href="#NPV" data-toc-modified-id="NPV-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>NPV</a></span><ul class="toc-item"><li><span><a href="#Compare-2-CashFlows-&amp;-Graph" data-toc-modified-id="Compare-2-CashFlows-&amp;-Graph-3.1"><span class="toc-item-num">3.1&nbsp;&nbsp;</span>Compare 2 CashFlows &amp; Graph</a></span></li></ul></li><li><span><a href="#Analyzing-Stock-Data" data-toc-modified-id="Analyzing-Stock-Data-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Analyzing Stock Data</a></span><ul class="toc-item"><li><span><a href="#Return-Comparision" data-toc-modified-id="Return-Comparision-4.1"><span class="toc-item-num">4.1&nbsp;&nbsp;</span>Return Comparision</a></span></li><li><span><a href="#RISK/Return/Graphy" data-toc-modified-id="RISK/Return/Graphy-4.2"><span class="toc-item-num">4.2&nbsp;&nbsp;</span>RISK/Return/Graphy</a></span></li></ul></li><li><span><a href="#Time-Series-Evaluation-&amp;-Forecasting" data-toc-modified-id="Time-Series-Evaluation-&amp;-Forecasting-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Time Series Evaluation &amp; Forecasting</a></span><ul class="toc-item"><li><span><a href="#Time-Series-Analysis-and-Forecasting" data-toc-modified-id="Time-Series-Analysis-and-Forecasting-5.1"><span class="toc-item-num">5.1&nbsp;&nbsp;</span>Time Series Analysis and Forecasting</a></span></li></ul></li><li><span><a href="#ims_pro_stock" data-toc-modified-id="ims_pro_stock-6"><span class="toc-item-num">6&nbsp;&nbsp;</span>ims_pro_stock</a></span><ul class="toc-item"><li><ul class="toc-item"><li><span><a href="#Comaritive-Analysis" data-toc-modified-id="Comaritive-Analysis-6.0.1"><span class="toc-item-num">6.0.1&nbsp;&nbsp;</span>Comaritive Analysis</a></span></li></ul></li></ul></li><li><span><a href="#PYFOLIO-TEAR" data-toc-modified-id="PYFOLIO-TEAR-7"><span class="toc-item-num">7&nbsp;&nbsp;</span>PYFOLIO TEAR</a></span></li><li><span><a href="#Get-stock-market-data-for-multiple-tickers" data-toc-modified-id="Get-stock-market-data-for-multiple-tickers-8"><span class="toc-item-num">8&nbsp;&nbsp;</span>Get stock market data for multiple tickers</a></span></li><li><span><a href="#Stock_Analysis_Income_Statement" data-toc-modified-id="Stock_Analysis_Income_Statement-9"><span class="toc-item-num">9&nbsp;&nbsp;</span>Stock_Analysis_Income_Statement</a></span></li><li><span><a href="#ASSET-PRICE-CHANGES-OVER-DATASET-TIME-VIA-GRAPH" data-toc-modified-id="ASSET-PRICE-CHANGES-OVER-DATASET-TIME-VIA-GRAPH-10"><span class="toc-item-num">10&nbsp;&nbsp;</span>ASSET PRICE CHANGES OVER DATASET TIME VIA GRAPH</a></span></li><li><span><a href="#Stock_Prediction_Model" data-toc-modified-id="Stock_Prediction_Model-11"><span class="toc-item-num">11&nbsp;&nbsp;</span>Stock_Prediction_Model</a></span><ul class="toc-item"><li><span><a href="#PREDICT-USING-MOVING-AVG¶" data-toc-modified-id="PREDICT-USING-MOVING-AVG¶-11.1"><span class="toc-item-num">11.1&nbsp;&nbsp;</span>PREDICT USING MOVING AVG¶</a></span></li><li><span><a href="#PLOT-PREDICTIONS-ON-DEV-SET" data-toc-modified-id="PLOT-PREDICTIONS-ON-DEV-SET-11.2"><span class="toc-item-num">11.2&nbsp;&nbsp;</span>PLOT PREDICTIONS ON DEV SET</a></span></li><li><span><a href="#FINAL-MODEL" data-toc-modified-id="FINAL-MODEL-11.3"><span class="toc-item-num">11.3&nbsp;&nbsp;</span>FINAL MODEL</a></span></li><li><span><a href="#FINDINGS" data-toc-modified-id="FINDINGS-11.4"><span class="toc-item-num">11.4&nbsp;&nbsp;</span>FINDINGS</a></span></li></ul></li><li><span><a href="#finance-test" data-toc-modified-id="finance-test-12"><span class="toc-item-num">12&nbsp;&nbsp;</span>finance test</a></span><ul class="toc-item"><li><span><a href="#Moving-Windows" data-toc-modified-id="Moving-Windows-12.1"><span class="toc-item-num">12.1&nbsp;&nbsp;</span>Moving Windows</a></span></li><li><span><a href="#Ordinary-Least-Squares-Regression-(OLS)" data-toc-modified-id="Ordinary-Least-Squares-Regression-(OLS)-12.2"><span class="toc-item-num">12.2&nbsp;&nbsp;</span>Ordinary Least-Squares Regression (OLS)</a></span></li><li><span><a href="#Building-A-Trading-Strategy-With-Python" data-toc-modified-id="Building-A-Trading-Strategy-With-Python-12.3"><span class="toc-item-num">12.3&nbsp;&nbsp;</span>Building A Trading Strategy With Python</a></span></li><li><span><a href="#Backtesting-A-Strategy" data-toc-modified-id="Backtesting-A-Strategy-12.4"><span class="toc-item-num">12.4&nbsp;&nbsp;</span>Backtesting A Strategy</a></span></li><li><span><a href="#Evaluating-Moving-Average-Crossover-Strategy" data-toc-modified-id="Evaluating-Moving-Average-Crossover-Strategy-12.5"><span class="toc-item-num">12.5&nbsp;&nbsp;</span>Evaluating Moving Average Crossover Strategy</a></span><ul class="toc-item"><li><span><a href="#SHARPE-RATIO:" data-toc-modified-id="SHARPE-RATIO:-12.5.1"><span class="toc-item-num">12.5.1&nbsp;&nbsp;</span>SHARPE RATIO:</a></span></li><li><span><a href="#Maximum-Drawdown" data-toc-modified-id="Maximum-Drawdown-12.5.2"><span class="toc-item-num">12.5.2&nbsp;&nbsp;</span>Maximum Drawdown</a></span></li></ul></li></ul></li><li><span><a href="#A-Simple-Time-Series-Analysis-Of-The-S&amp;P-500-Index" data-toc-modified-id="A-Simple-Time-Series-Analysis-Of-The-S&amp;P-500-Index-13"><span class="toc-item-num">13&nbsp;&nbsp;</span>A Simple Time Series Analysis Of The S&amp;P 500 Index</a></span></li><li><span><a href="#Asset_Beta_AND_Mkt_Beta" data-toc-modified-id="Asset_Beta_AND_Mkt_Beta-14"><span class="toc-item-num">14&nbsp;&nbsp;</span>Asset_Beta_AND_Mkt_Beta</a></span></li><li><span><a href="#Quantitative_Value_Strat" data-toc-modified-id="Quantitative_Value_Strat-15"><span class="toc-item-num">15&nbsp;&nbsp;</span>Quantitative_Value_Strat</a></span></li></ul></div>
"""

!pip install chart_studio quandl yfinance yahoofinancials alpha_vantage iexfinance patsy nsepy mpl_finance mplfinance



"""#### LIBRARY & PACKAGE IMPORT"""

# Commented out IPython magic to ensure Python compatibility.
### Data manipulation
import numpy as np
from bs4 import BeautifulSoup as bs
import pandas as pd
import pandas_datareader
from pandas_datareader import data as wb
from pandas.util.testing import assert_frame_equal
        ### Options for pandas
pd.options.display.max_columns = 50
pd.options.display.max_rows = 30
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()
        ### Cufflinks binds Plotly directly to pandas dataframes.
import cufflinks as cf
cf.go_offline(connected=True)

    ### Visualizations
import matplotlib.pyplot as plt
# %matplotlib inline
        ### More Visualizations
import chart_studio.plotly as py
import plotly
import plotly.tools as tls
import plotly.graph_objs as go
import plotly.offline as ply
plotly.offline.init_notebook_mode(connected=True)
        ### More Visualizations
import seaborn as sns
sns.set()
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.feature_selection import RFE
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error
from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error
from scipy.optimize import minimize
import statsmodels.tsa.api as smt
import statsmodels.api as sm
from tqdm import tqdm_notebook
from itertools import product

    ### IMPORT API FUNCTIONS
import quandl
import yfinance as yf
from yahoofinancials import YahooFinancials
import requests
import alpha_vantage
from alpha_vantage.timeseries import TimeSeries
import iexfinance
from iexfinance.stocks import Stock
from iexfinance.stocks import get_historical_data
from iexfinance.stocks import get_historical_intraday
from iexfinance.refdata import get_symbols
from iexfinance.data_apis import get_data_points
from iexfinance.data_apis import get_time_series
from iexfinance.altdata import get_social_sentiment
from iexfinance.altdata import get_ceo_compensation

    ### MISC LIB IMPORTS
import locale
from datetime import date, datetime, timedelta
from nsepy import get_history
from urllib.request import urlopen
import os
import time
import csv
import json
import requests
from patsy import dmatrices
import warnings
warnings.filterwarnings('ignore')

start = datetime(2017, 3, 30)
end = datetime(2020, 3, 30)

ticker = input('Ticker: ')

"""# CANDLE_STICK"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data as wb
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
# %matplotlib inline
import seaborn
from datetime import date, datetime, timedelta

#ticker = input('Ticker: ')
df = wb.get_data_yahoo(ticker, start = start)

#df1 = wb.get_data_yahoo('AMZN', start = start)
df.to_csv(r'C:/Users/gordon/PROGRAM//Data_Bank/MiscBank/1/_Dean_AXP.csv')
df=pd.read_csv(r'C:/Users/gordon/PROGRAM//Data_Bank/MiscBank/1/_Dean_AXP.csv')

df['Date1'] = pd.DataFrame(df['Date'])
df.set_index = df['Date1']
df['Date'] = pd.to_datetime(df['Date'])
df["Date"] = df["Date"].apply(mdates.date2num)

candle_data = df[['Date', 'Open', 'High', 'Low', 'Close']]

f1, ax = plt.subplots(figsize = (10,5))
candlestick_ohlc(ax,candle_data.values, colorup='green', colordown='red')
ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
plt.title('Candlestick Chart for stock1')
plt.xlabel('Date')
plt.ylabel('Value($)')
plt.show()

candle_data.head()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

############################################################
start = datetime(2017, 3, 30)
end = datetime(2020, 3, 30)

############################################################
#ticker = input('Ticker: ')
#ticker = 'AMZN'
df11 = wb.get_data_yahoo(ticker, start = start)
df11.to_csv(r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_101_1.csv')
df_11 = pd.read_csv(r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_101_1.csv')

############################################################
print('df & graph data')
df.plot()
plt.show()

############################################################
prices = df['Adj Close']
volumes = df['Volume']
    # The top plot consisting of daily closing prices
top = plt.subplot2grid((4, 4), (0, 0), rowspan=3, colspan=4)
top.plot(prices.index, prices, label='Last')
plt.title('ABN Last Price from 2015 - 2018')
plt.legend(loc=2)
    # The bottom plot consisting of daily trading volume
bottom = plt.subplot2grid((4, 4), (3,0), rowspan=1, colspan=4)
bottom.bar(volumes.index, volumes)
plt.title(str(ticker)+' Daily Trading Volume')
plt.gcf().set_size_inches(12, 8)
plt.subplots_adjust(hspace=0.75)
plt.show()

############################################################
# %matplotlib inline
import quandl
from mpl_finance import candlestick_ohlc
import matplotlib.dates as mdates
import matplotlib.pyplot as plt

df_subset = wb.get_data_yahoo(ticker, start = start)

df_subset['Date'] = df_subset.index.map(mdates.date2num)
df_ohlc = df_subset[['Date','Open', 'High', 'Low', 'Adj Close']]

figure, ax = plt.subplots(figsize = (8,4))
formatter = mdates.DateFormatter('%Y-%m-%d')
ax.xaxis.set_major_formatter(formatter)
candlestick_ohlc(ax, 
                 df_ohlc.values, 
                 width=0.8, 
                 colorup='green', 
                 colordown='red')
plt.show()

############################################################
df_11



"""# Compare_Returns"""

import numpy as np
import seaborn
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data as wb
from datetime import datetime


# Importing Commodity Prices
amzn_nasdaq_data = wb.DataReader("AMZN", data_source='yahoo',
                                 start=start, end=end)
amzn_nasdaq_data.to_csv(r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_102.csv')
AMZN_prices = pd.read_csv(r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_102.csv')
aapl_nasdaq_data = wb.DataReader("AAPL", data_source='yahoo',
                                 start=start, end=end)
aapl_nasdaq_data.to_csv(r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_1021.csv')
AAPL_prices = pd.read_csv(r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_1021.csv')

# Importing Stock Prices
nasdaq_data = wb.DataReader("NASDAQ100", "fred", start, end)
sap_data = wb.DataReader("SP500", "fred", start, end)


# Calculating Log Return
def log_return(prices):
    return np.log(prices/prices.shift(1))

AMZN_returns = log_return(AMZN_prices['Close'])
AAPL_returns = log_return(AAPL_prices['Close'])
nasdaq_returns = log_return(nasdaq_data['NASDAQ100'])
sap_returns = log_return(sap_data['SP500'])

# Comparing Return Volatility
def calculate_variance(dataset):
    mean = sum(dataset) / len(dataset)
    numerator = 0
    for data in dataset:
        numerator += (data - mean) ** 2
        variance = numerator / len(dataset)
    return variance

# DISPLAY AS PERCENTAGE
def display_as_percentage(val):
    return ('{:.4f} %'.format(val * 100))

print('.........Here are the Returns in Percentage: .............')
print('AMZN: ', display_as_percentage(AMZN_returns.var()))
print('AAPL: ', display_as_percentage(AAPL_returns.var()))
print('nasdaq: ', display_as_percentage(nasdaq_returns.var()))
print('sap: ', display_as_percentage(sap_returns.var()))

"""# NPV"""

import seaborn
import matplotlib.pyplot as plt

# Update Project Greenspace Cash Flows Here
project_a = [-2000000, 0, 0, 50000, 250000, 500000, 500000, 750000,
             750000, 500000, 500000, 500000, 500000, 500000, 1000000]

discount_rate = [0.05, 0.055, 0.06, 0.065, 0.07, 0.075, 0.08, 0.085,
                 0.09, 0.095, 0.1, 0.105, 0.11, 0.115, 0.12, 0.125, 
                 0.13, 0.135, 0.14, 0.145, 0.15]

def calculate_npv(rate, cash_flow):
    npv = 0
    for t in range(len(cash_flow)):
        npv += cash_flow[t]/(1+rate)**t
    return npv

npvs_a = list()
for rate in discount_rate:
    npv_a = calculate_npv(rate,project_a)
    npvs_a.append(npv_a)

plt.plot(discount_rate,npvs_a, linewidth = 3.0, 
         color = "green", label = "Project Greenspace")
plt.axhline(y=0, linewidth = 0.5, color = "black")
plt.title('NPV Profile: Project Greenspace ')
plt.xlabel('Discount Rate')
plt.ylabel('Net Present Value')
plt.legend()
plt.show()

"""## Compare 2 CashFlows & Graph"""

import seaborn
import matplotlib.pyplot as plt

# Update Project A Cash Flows Here
project_a = [-1000000, 0, 0, 50000, 50000, 200000, 250000, 
             250000, 250000, 250000, 375000, 375000, 375000,
             375000, 375000, 250000, 250000, 250000, 250000, 100000]

# Update Project B Cash Flows Here
project_b = [-1000000, 50000, 50000, 50000, 50000, 250000,
             500000, 500000, 500000, 500000, 100000, 100000,
             100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000]

discount_rate = [0.05, 0.055, 0.06, 0.065, 0.07, 0.075, 0.08,
                 0.085, 0.09, 0.095, 0.1, 0.105, 0.11, 0.115, 
                 0.12, 0.125, 0.13, 0.135, 0.14, 0.145, 0.15,
                 0.155, 0.16, 0.165, 0.17, 0.175, 0.18]

def calculate_npv(rate, cash_flow):
    npv = 0
    for t in range(len(cash_flow)):
        npv += cash_flow[t]/(1+rate)**t
    return npv

npvs_a = list()
npvs_b = list()
for rate in discount_rate:
    npv_a = calculate_npv(rate,project_a)
    npvs_a.append(npv_a)
    npv_b = calculate_npv(rate,project_b)
    npvs_b.append(npv_b)
    
plt.plot(discount_rate,npvs_a, linewidth = 2.0, color = "red",
         label = "Project A")
plt.plot(discount_rate,npvs_b, linewidth = 2.0, color = "blue",
         label = "Project B")
plt.axhline(y=0, linewidth = 0.5, color = "black")
plt.title('NPV Profile for Projects A and B')
plt.xlabel('Discount Rate')
plt.ylabel('Net Present Value')
plt.legend()
plt.show()

"""# Analyzing Stock Data
* You are given the monthly stock prices of two E-commerce companies, Amazon (AMZN) and eBay (EBAY). Help us analyze the risk and return for each investment! You will calculate the rates of return from this data, as well as other key statistics such as variance and correlation for assessing risk.

* Calculate Rate of Return
* Let’s start by calculating the logarithmic rates of return from the stock prices. Define a function called get_returns() 
* The function will eventually return a list of log returns calculated from each adjacent pair of prices. For now, create 
#### ...
* Are Amazon and eBay stock returns strongly or weakly correlated? Is the correlation positive or negative?
* The correlation coefficient between Amazon and eBay stock returns is around 0.67, so there is a moderate positive correlation. Stocks from the same industry tend to have a positive correlation because they are affected by similar external conditions.
* We should be careful about investing in highly correlated stocks to avoid putting all our eggs in one basket, so to speak.
* Instead, it is wise to invest in uncorrelated stocks, such that a loss in one does not automatically mean a loss in the other. This diversifies the investment portfolio and reduces overall risk.

## Return Comparision
"""

#*1***************************************************
from math import log, sqrt

# Calculate Log Return *******************************
def calculate_log_return(start_price, end_price):
    return log(end_price / start_price)

# Calculate Variance *******************************
def calculate_variance(dataset):
    mean = sum(dataset) / len(dataset)
    numerator = 0
    for data in dataset:
        numerator += (data - mean) ** 2
    return numerator / len(dataset)

# Calculate Standard Deviation *************************
def calculate_stddev(dataset):
    variance = calculate_variance(dataset)
    return sqrt(variance)

# Calculate Correlation Coefficient ********************
def calculate_correlation(set_x, set_y):
    sum_x = sum(set_x)
    sum_y = sum(set_y)
    sum_x2 = sum([x ** 2 for x in set_x])
    sum_y2 = sum([y ** 2 for y in set_y])
    sum_xy = sum([x * y for x, y in zip(set_x, set_y)])
    n = len(set_x)
    numerator = n * sum_xy - sum_x * sum_y
    denominator = sqrt((n * sum_x2 - sum_x ** 2) * (n * sum_y2 - sum_y ** 2))
    return numerator / denominator

#*******************************************************
# Write code here
def display_as_percentage(val):
    return '{:.1f}%'.format(val * 100)

amazon_prices = [1699.8, 1777.44, 2012.71, 2003.0, 1598.01, 1690.17, 1501.97, 1718.73, 1639.83, 1780.75,
                 1926.52, 1775.07, 1893.63]
amazon_time=['JAN','FEB','MAR','APRIL','MAY','JUNE','JULY','AUG','SEP','OCT','NOV','DEC','13th month']
ebay_prices = [35.98, 33.2, 34.35, 32.77, 28.81, 29.62, 27.86, 33.39, 37.01, 37.0, 38.6, 35.93, 39.5]


#*2*3*4*5*6*********************************************
# Write code here
def get_returns(prices):
    returns=[]
    for i in range(1,13,1):
        start_price = prices[i-1]
        end_price = prices[i]
        log_return = calculate_log_return(start_price, end_price)
        returns.append(log_return)      
    return returns

amazon_monthly_returns = get_returns(amazon_prices)
ebay_monthly_returns = get_returns(ebay_prices)

print('Amazons Monthly returns are: ' +str([display_as_percentage(amazon_monthly_return) for
                                            amazon_monthly_return in amazon_monthly_returns]))
print('')
print('Ebays Montly returns are: ' +str([display_as_percentage(ebay_monthly_return) for 
                                         ebay_monthly_return in ebay_monthly_returns]))
print('')

#*7*****************************************************
def annualize_return(log_return):
    return sum(log_return)

annual_return_amazon=display_as_percentage(annualize_return(amazon_monthly_returns))
print('Amazons Annual Return is: ' + str(annual_return_amazon))

annual_return_ebay=display_as_percentage(annualize_return(ebay_monthly_returns))
print('Ebays Annual Return is: ' + str(annual_return_ebay))
print('')

#*8*****************************************************
amzn_var=display_as_percentage(calculate_variance(amazon_monthly_returns))
print('Amazons Monthly Variance is: '+str(amzn_var))
ebay_var=display_as_percentage(calculate_variance(ebay_monthly_returns))
print('Ebays Monthly Variance is: '+str(ebay_var))
print('')

#*9*****************************************************
amzn_stddev=display_as_percentage(calculate_stddev(amazon_monthly_returns))
print('Amazons Standard Deviation is: '+ str(amzn_stddev))

ebay_stddev=display_as_percentage(calculate_stddev(ebay_monthly_returns))
print('Amazons Standard Deviation is: '+ str(ebay_stddev))
print('')

#*10****************************************************
corr_coef=round((calculate_correlation(amazon_monthly_returns, ebay_monthly_returns)),2)
print('Amazon and Ebays Correlation Coefficient is: '+str(corr_coef))

from matplotlib import pyplot as plt

plt.plot(amazon_time, amazon_prices)
plt.show()

"""## RISK/Return/Graphy"""

import seaborn
import numpy as np
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data as wb
import matplotlib.pyplot as plt

ticker = 'AAPL'
#ticker = input('Ticker: ')

df1 = wb.get_data_yahoo('AMZN', start = start)
df1.to_csv(r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_104.csv')
df = pd.read_csv(r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_104.csv')

df['Daily_Log_RoR'] = np.log(df['Adj Close']/df['Adj Close'].shift(1))
stdev = np.std(df['Daily_Log_RoR'])


plt.hist(df['Daily_Log_RoR'].dropna())
plt.title('Histogram of '+str(ticker)+' Daily_Log_RoR: ')
plt.xlabel('Log Rate of Return')
plt.ylabel('Number of Days')
plt.show()


def display_as_percentage(val):
    return '{:.3f}%'.format(val * 100)

print('Standard Deviation Of: '+str(ticker)+' = 'display_as_percentage(stdev))
df = df.drop(['High','Low','Open','Close'], axis=1)
df

"""# Time Series Evaluation & Forecasting"""

# Commented out IPython magic to ensure Python compatibility.
######################################## LIB IMPORT
import numpy as np
import pandas as pd
import pandas_datareader as pdr
from pandas_datareader import data as wb
import matplotlib.pyplot as plt
# %matplotlib inline
from statsmodels.tsa.holtwinters import SimpleExpSmoothing as ses

################################################## FUNCTIONS
def display_as_percentage(val):
    return '{:.3f}%'.format(val * 100)

for i in range(10):
    model=ses(stock_W['Close'])
    stock_W['Forecast']=model.fit(alpha).fittedvalues
    stock_W['MSE_W'].mean()
    values['alpha'][i]=alpha
    values['MSE_W'][i]=MSE_W
    alpha+=0.1
    
for i in range(10):
    model_M=ses(stock_M['Close'])
    stock_M['Forecast']=model.fit(alpha).fittedvalues
    stock_M['MSE_M'].mean()
    values_M['alpha'][i]=alpha_M
    values_M['MSE_M'][i]=MSE_M
    alpha_M+=0.1

############################################## PULL DATA
# ticker = input('Ticker: ')
stock = wb.get_data_yahoo(ticker, start = start, end = end)
stock = stock.drop(['Adj Close','Volume','Open',
                    'Low','High'], axis=1)

######################################## moving average
stock['5.day']=stock['Close'].rolling(5).mean().shift()
stock['MAD.5']=np.abs(stock['Close']-stock['5.day'])

stock['21.day']=stock['Close'].rolling(21).mean().shift()
stock['MAD.10']=np.abs(stock['Close']-stock['21.day'])

stock['63.day']=stock['Close'].rolling(63).mean().shift()
stock['MAD.30']=np.abs(stock['Close']-stock['63.day'])

stock['126.day']=stock['Close'].rolling(126).mean().shift()
stock['MAD.90']=np.abs(stock['Close']-stock['126.day'])

stock['252.day']=stock['Close'].rolling(252).mean().shift()
stock['MAD.90']=np.abs(stock['Close']-stock['252.day'])
    ################## PLOT 5/21/63/126/252
print('Moving Average vs Close: 5/21/63/126/252')
plt.figure(figsize=(13,12))
plt.plot(stock['5.day'],'--', label='5.day')
plt.plot(stock['21.day'],'--', label='21.day')
plt.plot(stock['63.day'],'--', label='63.day')
plt.plot(stock['126.day'],'--', label='126.day')
plt.plot(stock['252.day'],'--', label='252.day')
plt.plot(stock['Close'], label='Close')
plt.legend()
plt.show()
with plt.style.context('ggplot'):
    plt.figure(figsize=(13,12))
    plt.plot(stock.Close,label='Close')
    plt.plot(stock['5.day'], label='5 day')
    plt.plot(stock['21.day'], label='21 day')
    plt.plot(stock['63.day'], label='63 day')
    plt.plot(stock['126.day'], label='126 day')
    plt.plot(stock['252.day'], label='252 day')
    plt.legend()
    plt.show()
    ############################## PLOT 21/63/126/252
print('Moving Average vs Close: 21/63/126/252')
plt.figure(figsize=(13,12))
plt.plot(stock['21.day'],'--', label='21.day')
plt.plot(stock['63.day'],'--', label='63.day')
plt.plot(stock['126.day'],'--', label='126.day')
plt.plot(stock['252.day'],'--', label='252.day')
plt.plot(stock['Close'], label='Close')
plt.legend()
plt.show()
with plt.style.context('ggplot'):
    plt.figure(figsize=(13,12))
    plt.plot(stock.Close,label='Close')
    plt.plot(stock['21.day'], label='21 day')
    plt.plot(stock['63.day'], label='63 day')
    plt.plot(stock['126.day'], label='126 day')
    plt.plot(stock['252.day'], label='252 day')
    plt.legend()
    plt.show()      
    ############################## PLOT 5/63/252
print('Moving Average vs Close: 5/63/252')
plt.figure(figsize=(13,12))
plt.plot(stock['5.day'],'--', label='5.day')
plt.plot(stock['63.day'],'--', label='63.day')
plt.plot(stock['252.day'],'--', label='252.day')
plt.plot(stock['Close'], label='Close')
plt.legend()
plt.show()
with plt.style.context('ggplot'):
    plt.figure(figsize=(13,12))
    plt.plot(stock.Close,label='Close')
    plt.plot(stock['5.day'], label='5 day')
    plt.plot(stock['63.day'], label='63 day')
    plt.plot(stock['252.day'], label='252 day')
    plt.legend()
    plt.show()

################################# PULL HISTORICAL DATA DAILY
stock = pdr.get_data_yahoo(ticker, start=start, end=end)
stock.drop(['Adj Close','Volume'], axis=1, inplace=True)
    # RESAMPLE IN WEEKLY DATA
stock_W = pd.DataFrame(stock.resample('W').last())
    # RESAMPLE IN MONTHLY DATA
stock_M = pd.DataFrame(stock.resample('M').last())
    ############################# MODELING:
    # MODEL WEEKLY DATASET
model = ses(stock_W['Close'])
model_fit_W=model.fit()
model_fit_W.predict()
        # WEEKLY
y_hat_W=model_fit_W.fittedvalues
    # MODEL MONTHLY DATASET
model = ses(stock_M['Close'])
model_fit_M=model.fit()
model_fit_M.predict()
        # MONTHLY
y_hat_M=model_fit_M.fittedvalues

################################## MEAN ABSOLUTE DEVIATION
stock_W['W']=stock_W['Close'].rolling(5).mean().shift()
stock_W['MAD_W']=np.abs(stock_W['Close']-stock_W['W'])

stock_M['M']=stock_M['Close'].rolling(5).mean().shift()
stock_M['MAD_M']=np.abs(stock_M['Close']-stock_M['M'])
    ##################################
    # MEAN PERCENT ERROR
stock_W['MAPE_W']=stock_W['MAD_W']/stock_W['Close']
stock_M['MAPE_M']=stock_M['MAD_M']/stock_M['Close']
    # MEAN SQUARED ERROR
stock_W['MSE_W']=stock_W['MAD_W']**2
MSE_W = stock_W['MSE_W'].mean()
    # MEAN 
stock_M['MSE_M']=stock_M['MAD_M']**2
MSE_M = stock_M['MSE_M'].mean()
    ############################
    # WEEKLY
RMSE_W = np.sqrt(MSE_W)
model_fit_W.params
print('The RMSE_W Is: '+display_as_percentage(RMSE_W))
    # MONTHLY
RMS_M = np.sqrt(MSE_M)
model_fit_M.params
print('The RMS_M Is: '+display_as_percentage(RMS_M))
    ############################
alpha = 0.1
alpha_M = 0.1
values = pd.DataFrame({'alpha': np.zeros(10), 
                      'MSE_W': np.zeros(10)})
values_M= pd.DataFrame({'alpha': np.zeros(10), 
                        'MSE_M': np.zeros(10)})

#################################### MOVING AVG 5/50/200
df = pdr.get_data_yahoo(ticker,start = start, end=end)
df.drop(['Adj Close','Volume','High',
         'Low','Open'], axis=1, inplace=True)

df['5day']=df['Close'].rolling(5).mean()
df['50day']=df['Close'].rolling(50).mean()
df['200day']=df['Close'].rolling(200).mean()
df['Change']=np.log(df.Close/df.Close.shift())

print('Moving Average vs Close: 5/50/200')
with plt.style.context('ggplot'):
    plt.figure(figsize=(10,10))
    plt.plot(df.Close)
    plt.plot(df['5day'], label='5_Day')
    plt.plot(df['50day'], label='50_Day')
    plt.plot(df['200day'], label='200_Day')
    plt.legend(loc=2)
    plt.show

df['position']=np.where(df['5day']>df['50day'],1,0)
df['position']=np.where(df['5day']<df['50day'],-1,
                        df['position'])
df['position']=np.where(df['50day']>df['200day'],1,0)
df['position']=np.where(df['50day']<df['200day'],-1,
                        df['position'])
df['position']=np.where(df['5day']>df['200day'],1,0)
df['position']=np.where(df['5day']<df['200day'],-1,
                        df['position'])
df['system']=df['position']*df['Change']
df[['Change','system']].cumsum().plot()

"""## Time Series Analysis and Forecasting"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import pandas_datareader as pdr
import statsmodels.api as sm
import datetime as datetime
import matplotlib.pyplot as plt
# %matplotlib inline
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller

############################################## PULL DATA
# ticker = input('Ticker: ')
##############################################
df1 = wb.get_data_yahoo(ticker, start = start, end=end)
df1.to_csv(r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_105.csv')
df = pd.read_csv(r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_105.csv')

##############################################
#df = stock.drop(['Adj Close','Volume','Open','Low','High'], axis=1)
df['Date1']=df['Date']
df.set_index('Date1')
timeseries = df['Open']

##############################################
timeseries.rolling(12).mean().plot(label='12 Month Rolling Mean')
timeseries.rolling(12).std().plot(label='12 Month Rolling Std')
timeseries.plot()
plt.legend()

##############################################
decomposition = seasonal_decompose(df['Open'], freq=12)  
fig = plt.figure()  
fig = decomposition.plot()  
fig.set_size_inches(13, 8)

##############################################
def adf_check(time_series):
    """
    Pass in a time series, returns ADF report
    """
    result = adfuller(time_series)
    print('Augmented Dickey-Fuller Test:')
    labels = ['ADF Test Statistic','p-value','#Lags Used',
              'Number of Observations Used']
    for value,label in zip(result,labels):
        print(label+' : '+str(value) )
    if result[1] <= 0.05:
        print("strong evidence against the null hypothesis, \
              reject the null hypothesis. Data has no unit root and \
              is stationary")
    else:
        print("weak evidence against null hypothesis, \
        time series has a unit root, indicating it is non-stationary ")
        
adf_check(df['Open'])
df['Open First Difference'] = df['Open'] - df['Open'].shift(1)
adf_check(df['Open First Difference'].dropna())
df['Open First Difference'].plot()
print('')

##############################################
fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
fig = sm.graphics.tsa.plot_acf(df['Open First Difference'].iloc[13:], 
                               lags=40, ax=ax1)
ax2 = fig.add_subplot(212)
fig = sm.graphics.tsa.plot_pacf(df['Open First Difference'].iloc[13:],
                                lags=40, ax=ax2)

##############################################
model = sm.tsa.statespace.SARIMAX(df['Open'],order=(0,1,0),
                                  seasonal_order=(0,1,0,12))             
results = model.fit()          
print(results.summary())

##############################################
df['forecast'] = results.predict(start = 400, end= 700, dynamic= True) 
df[['Open','forecast']].plot(figsize=(12,8))
plt.title('FORECAST!!!')
plt.ylabel('Price')
plt.xlabel('Date')

"""# ims_pro_stock"""

#Declaring Start Date and End Date
    ##scraping stock data of company using yahoo finance
        # (cna use google or morning star)
    
def comp_scraper(company_name):
    df = wb.DataReader(company_name, 'yahoo', start, end)
    df['comp']=company_name
    return (df)

data_df=pd.DataFrame()

#Data scraping for Apple,IBM,Microsoft,Walmart
company_list=['TPL', 'AMZN', 'MSFT', 'WMT', 'TSLA']
tic = company_list[0]

for company in company_list:
    df=comp_scraper(company)
    data_df=data_df.append(df)
    
##checking shape of data frame
print(data_df.shape)
print(data_df.columns)

# Statastics
data_df.describe()

########## plotting close price of ea. Stock for Analysis
for company in company_list:
    x = data_df[data_df['comp']==company]
    x['Close'].plot(figsize=(10, 7))
# Define the label for the title of the figure
    plt.title("Close Price of %s" % company, fontsize=16)
# Define the labels for x-axis and y-axis
    plt.ylabel('Price', fontsize=14)
    plt.xlabel('Year', fontsize=14)
# Plot the grid lines
    plt.grid(which="major", color='k', linestyle='-.', linewidth=0.5)
# Show the plot
    plt.show()

"""### Comaritive Analysis"""

##########################

# ticker = input('Ticker: ')
#tickers_list = ['AAPL', 'IBM', 'MSFT', 'WMT']

data1 = pd.DataFrame()
data1 = wb.get_data_yahoo(tickers_list, start = start, end = end)
#data1 = stock.drop(['Adj Close','Volume','Open','Low','High'], axis-1)

Alpha = ((data1['Adj Close'].pct_change()+1).cumprod())
Alpha.plot(figsize=(10, 7))
    # Show the legend
plt.legend()
    # Define the label for the title of the figure
plt.title("Close Price", fontsize=16)
    # Define the labels for x-axis and y-axis
plt.ylabel('Price', fontsize=14)
plt.xlabel('Year', fontsize=14)
    # Plot the grid lines
plt.grid(which="major", color='k', linestyle='-.', linewidth=0.5)
plt.show()
print(tic)

"""#### function to predict stock for each company########"""

def predict(company_name):
    data_comp=data_df[data_df['comp']==company_name]
    data = data_comp.sort_index(ascending=True, axis=0)
    data['Date']=data.index
#creating a separate dataset
    new_data = pd.DataFrame(index=range(0,len(data)),columns=['Date', 'Close'])

    for i in range(0,len(data)):
        new_data['Date'][i] = data['Date'][i]
        new_data['Close'][i] = data['Close'][i]
    ###creating features
    new_data.index=new_data['Date']
    new_data['days']=new_data['Date'].apply(lambda x:x.weekday())

    new_data['year']=new_data['Date'].apply(lambda x:x.year)

    new_data['month']=new_data['Date'].apply(lambda x:x.month)
    new_data['day']=new_data['Date'].apply(lambda x:x.day)
    new_data['Dayofyear']=new_data['Date'].apply(lambda x:x.dayofyear)
    #new_data['Dayofweek']=new_data['Date'].apply(lambda x:x.dayofweek)


    new_data['quarter']=new_data['Date'].apply(lambda x:x.quarter)

    new_data['semester'] = np.where(new_data.quarter.isin([1,2]),1,2)
    new_data['weekofyear']=new_data['Date'].apply(lambda x:x.weekofyear)
    new_data.drop("Date",1,inplace=True)
    x=int(0.7*len(new_data))
    train = new_data[:x]
    valid = new_data[x:]

    x_train = train.drop('Close', axis=1)
    y_train = train['Close']
    x_valid = valid.drop('Close', axis=1)
    y_valid = valid['Close']

#implement linear regression
    from sklearn.linear_model import LinearRegression
    model = LinearRegression()
    model.fit(x_train,y_train)
    preds = model.predict(x_valid)
    rms=np.sqrt(np.mean(np.power((np.array(y_valid)-np.array(preds)),2)))
    print(rms)
    valid['Predictions'] = 0
    valid['Predictions'] = preds
    valid.index = new_data[x:].index
    train.index = new_data[:x].index
    plt.figure(figsize=(5,5))
    plt.plot(train['Close'],label=company_name)
    plt.plot(valid[['Close', 'Predictions']])
    plt.xlabel("year")
    plt.ylabel("Close")
    plt.xticks(rotation=45)
    plt.legend()

for company in company_list:
    predict(company)

"""#### filter data of FIRST TICKER"""

df_1ST = data_df[data_df['comp']=='TPL']

def fill_null_values(dataset, value):
    nulls = pd.DataFrame(dataset.isnull().sum().sort_values(ascending=False))
    nulls = nulls[nulls>0]
    nulls.columns = ['Null Count']
    nulls.index.name = 'Feature'
    nulls

    for column in nulls[nulls["Null Count"]> 0].index:
        if dataset[column].dtype == np.number and value == 0:
            dataset[column].fillna(dataset[column].mean(), inplace = True)
        elif value == 1:
            dataset[column].fillna(dataset[column].median(), inplace = True)
        else:
            dataset[column].fillna("NA", inplace = True)
    print(dataset.isnull().sum())
    return dataset

predictors = fill_null_values(df_1ST,0)

#print(predictors.head())
#print(df_1ST.info)
df_1ST.describe()

##plotting close price
plt.plot(df_1ST['Close'])
plt.xticks(rotation=45)
print('plotting close price of: '+str(tic))

"""#### Outlier Detection
#### will not replace or remove Outlier as their may be some reason or event...
"""

def detect_outliers(dataframe):
    cols = list(dataframe)
    
    for column in cols:
        if column in dataframe.select_dtypes(include=np.number).columns:
            q1 = dataframe[column].quantile(0.25)
            q3 = dataframe[column].quantile(0.75)
            iqr = q3 - q1
            fence_low = q1 - (1.5*iqr)
            fence_high = q3 + (1.5*iqr)

            print(column + ' ---------', dataframe.loc[(dataframe[column] < fence_low) | (dataframe[column] > fence_high)].shape[0])

detect_outliers(df_1ST)

df_1ST['Date']=df_1ST.index

sns.relplot(x='Date', y="Volume",kind="line", data=df_1ST)

"""#### Feature Creation using date, dropping all other columns"""

data = df_1ST.sort_index(ascending=True, axis=0)
#creating a separate dataset
new_data = pd.DataFrame(index=range(0,len(data)),columns=['Date', 'Close'])

for i in range(0,len(data)):
    new_data['Date'][i] = data['Date'][i]
    new_data['Close'][i] = data['Close'][i]
    ###creating features
new_data['days']=new_data['Date'].apply(lambda x:x.weekday())
new_data['year']=new_data['Date'].apply(lambda x:x.year)
new_data['month']=new_data['Date'].apply(lambda x:x.month)
#new_data['day']=new_data['Date'].apply(lambda x:x.day)
new_data['Dayofyear']=new_data['Date'].apply(lambda x:x.dayofyear)
new_data['Dayofweek']=new_data['Date'].apply(lambda x:x.dayofweek)
new_data['quarter']=new_data['Date'].apply(lambda x:x.quarter)
new_data['semester'] = np.where(new_data.quarter.isin([1,2]),1,2)
new_data['weekofyear']=new_data['Date'].apply(lambda x:x.weekofyear)
#new_data.drop("Date",1,inplace=True)

sns.catplot(x='quarter', y="Close", data=new_data)
print('')

new_data['Close']=new_data['Close'].astype(float)

sns.catplot(x="year", y="Close", kind="box", data=new_data)

"""#### outliers in year 2017###"""

sns.catplot(x="year", y="Close", hue="quarter",
            kind="violin", data=new_data)

sns.catplot(x="month", y="Close", hue="year", kind="point", data=new_data)

sns.catplot(x="quarter", y="Close", hue="year", kind="point", data=new_data)

new_data=new_data.set_index("Date")

def correlation_heatmap(df):
    _ , ax = plt.subplots(figsize =(14, 12))
    colormap = sns.diverging_palette(220, 10, as_cmap = True)
    
    _ = sns.heatmap(
        df.corr(), 
        cmap = 'viridis',
        square=True, 
        cbar_kws={'shrink':.9 }, 
        ax=ax,
        annot=True, 
        linewidths=0.1,vmax=1.0, linecolor='white',
        annot_kws={'fontsize':12 }
    )
    
    plt.title('Pearson Correlation of Features', y=1.05, size=15)
    
    correlation = df.corr().unstack().sort_values(kind='quicksort')
    print('Highly Correlated Variables')
    return correlation[((correlation>=0.75) | (correlation<=-0.75)) & (correlation!=1)]

correlation_heatmap(new_data)

"""#### Baseline Model"""

def run_model(dataframe, model):
    '''
    Performs model training and tests using ROC-AUC 
    returns AUC score
    '''
    dataframe=dataframe.sort_index(ascending=True, axis=0)
    x=int(0.7*len(dataframe))
    train = dataframe[:x]
    valid = dataframe[x:]

    x_train = train.drop('Close', axis=1)
    y_train = train['Close']
    x_valid = valid.drop('Close', axis=1)
    y_valid = valid['Close']
    model.fit(x_train, y_train)
    preds = model.predict(x_valid)
    valid['Predictions'] = 0
    valid['Predictions'] = preds
    valid.index = dataframe[x:].index
    train.index = dataframe[:x].index
    
   
    print("Stock forcast for "+str(tic)+"  using "+str(model))
    plt.plot(train['Close'])
    plt.plot(valid[['Close','Predictions']])
    plt.xlabel("year")
    plt.ylabel("Price")
    plt.xticks(rotation=45)
    #plt.legend()
    plt.show()
    
###Applying Models
models = [LinearRegression,SVR] #KNeighborsRegressor]
for model in models:
    # run model
    model = model()
    run_model(new_data, model) # train and returns AUC test score

"""#### Feature Selection"""

def feature_selection(dataframe,target,number_of_features,model):
    X = dataframe
    y = target

    models = model()
    rfe = RFE(models,number_of_features)
    rfe = rfe.fit(X,y)
    print('Features  to be selected for {}'.format(str(models)))
    features = pd.Series(rfe.ranking_, index=X.columns)
    print(features[features.values==1].index.tolist())
    print('===='*30)

# Choosing the models
models = [LinearRegression,SVR]


X_train=new_data.drop("Close",1)
y_train=new_data['Close']
# Selecting 8 number of features
for i in models:
    feature_selection(X_train,y_train,8,i)

"""#### Hypertuning using GridSearchCV on Linear Regression"""

# Commented out IPython magic to ensure Python compatibility.
### Using first 8 features
new_data=new_data[['days', 'year', 'month', 'Dayofyear', 'Dayofweek',
                   'quarter', 'semester', 'weekofyear',"Close"]]
new_data.columns

def gridcv(dataframe):
    dataframe=dataframe.sort_index(ascending=True, axis=0)
    x=int(0.7*len(dataframe))
    train = dataframe[:x]
    valid = dataframe[x:]

    x_train = train.drop('Close', axis=1)
    y_train = train['Close']
    x_valid = valid.drop('Close', axis=1)
    y_valid = valid['Close']

    model =LinearRegression()
    parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}
    grid = GridSearchCV(model,parameters, cv=None)
    grid.fit(x_train, y_train)
    preds=grid.predict(x_valid)
    print ("r2 / variance : ", grid.best_score_)
    print("Residual sum of squares: %.2f"
#               % np.mean((grid.predict(x_valid) - y_valid) ** 2))
    valid['Predictions'] = 0
    valid['Predictions'] = preds
    valid.index = dataframe[x:].index
    train.index = dataframe[:x].index
    
   
    plt.title("Close Price")
    plt.plot(train['Close'])
    plt.plot(valid[['Close','Predictions']])
    plt.xlabel("year")
    plt.ylabel("Price")
    plt.xticks(rotation=45)
    #plt.legend()
    plt.show()

gridcv(new_data)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
from datetime import *
import pandas_datareader.data as data
from pandas.util.testing import assert_frame_equal

start = datetime(2016, 12, 31)
end = datetime.now()
stock_data = data.DataReader(["SBUX", "SHOP", "BIDU", "WDAY", "WIX",],
                             'yahoo', start, end)

def stock_predict(ticker):
    ## Chose to start from 1995, default end is today's date
    #df = web.get_data_yahoo(ticker, start='1995-01-01')

    today = dt.date.today()
    one_year = dt.timedelta(days=365)
    one_year_ago = today - one_year

    quandl.ApiConfig.api_key = 'khgNVC9qySoZyPshAWaw'
    data_df = quandl.get('WIKI/' + ticker, start_date=str(one_year_ago), 
                         end_date=str(today))
    data_price = data_df['Close'].values
    data_np = data_df.values
    #print(data_price)
    #print(data_price[0])


    z_data = quandl.get_table('ZACKS/FC', ticker='AAPL')
    print(z_data)

    #data_price.plot();
    #plt.show()

    #period = 50
    #train = 0.8
    #split = int(train*len(df))
    
# stock_data


stock_data_closing_prices = stock_data['Adj Close']
stock_data_closing_prices.plot()
plt.xlabel("Date")
plt.ylabel("Adjusted Closing Price")
plt.title("Tech Stocks Adjusted Price Over Time")
plt.show()


fig = plt.figure(figsize=(15,15))
ax1 = fig.add_subplot(321)
ax2 = fig.add_subplot(322)
ax3 = fig.add_subplot(323)
ax4 = fig.add_subplot(324)
ax5 = fig.add_subplot(325)



ax1.plot(stock_data['Adj Close']['SBUX'].pct_change())
ax1.set_title("SBUX")
ax2.plot(stock_data['Adj Close']['SHOP'].pct_change())
ax2.set_title("SHOP")
ax3.plot(stock_data['Adj Close']['BIDU'].pct_change())
ax3.set_title("BIDU")
ax4.plot(stock_data['Adj Close']['WDAY'].pct_change())
ax4.set_title("WDAY")
ax5.plot(stock_data['Adj Close']['WIX'].pct_change())
ax5.set_title("WIX")
plt.tight_layout()
plt.show()

stock_data_daily_returns = stock_data['Close'].pct_change()
stock_data_daily_returns.plot()
plt.xlabel("Date")
plt.ylabel("ROR")
plt.title("Daily Simple Rate of Return Over time")
plt.figure(figsize=(16,9))
plt.show()

# calculate daily mean
daily_mean = stock_data_daily_returns.mean()
daily_mean

# daily mean index for the x axis
daily_mean.keys()


# grab each daily mean value for the y axis
height = []
for key in daily_mean.keys():
    height.append(daily_mean[key])
height


# arrange keys on x axis based on length
x_pos = np.arange(len(daily_mean.keys()))
x_pos


# plot bars
plt.bar(x_pos, height)
 
# create names on the x-axis
plt.xticks(x_pos, daily_mean.keys())

# label chart
plt.xlabel("Tech_Stocks")
plt.ylabel("daily mean")
plt.title("daily mean rate of return")

# show graphic
plt.show()

# calculate variance
daily_var = stock_data_daily_returns.var()
daily_var


# variance index for the x axis
daily_var.keys()

# grab each variance value for the y axis
height = []
for key in daily_var.keys():
    height.append(daily_var[key])
height

# arrange keys on x axis based on length
x_pos = np.arange(len(daily_var.keys()))
x_pos

# plot bars
plt.bar(x_pos, height)
 
# create names on the x-axis
plt.xticks(x_pos, daily_var.keys())

# label chart
plt.xlabel("Tech_Stocks")
plt.ylabel("variance")
plt.title("daily variance")

# show graphic
plt.show()

# calculate standard deviation
daily_std = stock_data_daily_returns.std()
daily_std


# standard deviation index for the x axis
daily_var.keys()


# grab each standard deviation value for the y axis
height = []
for key in daily_std.keys():
    height.append(daily_std[key])
height

# arrange keys on x axis based on length
x_pos = np.arange(len(daily_std.keys()))
x_pos

# plot bars
plt.bar(x_pos, height)
 
# create names on the x-axis
plt.xticks(x_pos, daily_std.keys())

# label chart
plt.xlabel("Tech_Stocks")
plt.ylabel("std")
plt.title("daily std")

# show graphic
plt.show()

stock_data_daily_returns.corr()

"""# PYFOLIO TEAR"""

import pyfolio as pf
from pandas.util.testing import assert_frame_equal


# Define the ticker list
tickers_list = ['AAPL', 'AMZN', 'MSFT', 'WMT']

# Import pandas and create a placeholder for the data
import pandas as pd
data = pd.DataFrame(columns=tickers_list)

# Feth the data
import yfinance as yf
for ticker in tickers_list:
    data[ticker] = yf.download(ticker, period='5y',)['Adj Close']

# Compute the returns of individula stocks and then compute the daily mean returns.
# The mean return is the daily portfolio returns with the above four stocks.
data = data.pct_change().dropna().mean(axis=1)

# Print first 5 rows of the data
data

pf.create_full_tear_sheet(data)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

"""# Get stock market data for multiple tickers
To get the stock market data of multiple stock tickers, you can create a list of tickers and call the quandl get method for each stock ticker.[1]

For simplicity, I have created a dataframe data to store the adjusted close price of the stocks.
"""

# Define the ticker list
tickers_list = ['AAPL', 'IBM', 'MSFT', 'WMT']

# Import pandas
data = pd.DataFrame(columns=tickers_list)

# Feth the data
for t in tickers_list:
    data[t] = pdr.DataReader(t, 'yahoo', start, end)['Close']

    
# Plot all the close prices
data.plot(figsize=(10, 7))
# Show the legend
plt.legend()
# Define the label for the title of the figure
plt.title("Adjusted Close Price", fontsize=16)
# Define the labels for x-axis and y-axis
plt.ylabel('Price', fontsize=14)
plt.xlabel('Year', fontsize=14)
# Plot the grid lines
plt.grid(which="major", color='k', linestyle='-.', linewidth=0.5)
plt.show()


# Print first 5 rows of the data
data.head()

"""# Stock_Analysis_Income_Statement"""

import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import requests
import json

def selectquote(quote): 
    r= requests.get(f"https://financialmodelingprep.com/api/v3/financials\
    /income-statement/{quote}?period=quarter") 
    r = r.json() 
    stock = r['financials']
    stock = pd.DataFrame.from_dict(stock) 
    stock = stock.T   
    stock.columns = stock.iloc[0]
    stock.reset_index(inplace=True) 
    return stock 

selectquote('AAPL')

incomeStatement = selectquote('AAPL')
incomeStatement

def selectquote1(quote):
    r= requests.get(f"https://financialmodelingprep.com/api/v3/financials\
    /income-statement/{quote}?period=quarter")
    r = r.json()
    stock = r['financials']
    stock = pd.DataFrame.from_dict(stock)
    stock = stock.T
    stock.columns = stock.iloc[0]
    stock.reset_index(inplace=True)
    stock = stock.iloc[:,0:2]
    stock.rename(columns={ stock.columns[1]: quote }, inplace = True)
    cols = stock.columns.drop('index')
    stock = stock.iloc[1:,]
    return stock

selectquote1('AAPL')

"""# ASSET PRICE CHANGES OVER DATASET TIME VIA GRAPH"""

tickers_list = ['AAPL', 'IBM', 'MSFT', 'WMT']
#s_data = pd.DataReader()

fig = plt.figure(figsize=(15,15))
ax1 = fig.add_subplot(321)
ax2 = fig.add_subplot(322)
ax3 = fig.add_subplot(323)
ax4 = fig.add_subplot(324)
#ax5 = fig.add_subplot(325)
#ax6 = fig.add_subplot(326)

s_data = wb.DataReader(['AAPL', 'IBM', 'MSFT', 'WMT'], 'yahoo', start, end)

ax1.plot(s_data['Adj Close']['AAPL'])
ax1.set_title(tickers_list[0])
ax2.plot(s_data['Adj Close']['IBM'])
ax2.set_title(tickers_list[1])
ax3.plot(s_data['Adj Close']['MSFT'])
ax3.set_title(tickers_list[2])
ax4.plot(s_data['Adj Close']['WMT'])
ax4.set_title(tickers_list[3])

plt.xlabel('Date')
plt.ylabel('Adjusted Closing Price')
plt.title('All Securities Adj. Price: ')
plt.tight_layout()
plt.show()

"""# Stock_Prediction_Model"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import math
import matplotlib
import numpy as np
import pandas as pd
import seaborn as sns
import time
from datetime import date, datetime, time, timedelta
from matplotlib import pyplot as plt
from pylab import rcParams
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from tqdm import tqdm_notebook
np.warnings.filterwarnings('ignore')

#### Input params ##################
stk_path = (r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_1011.csv')

test_size = 0.2                 # proportion of dataset to be used as test set
cv_size = 0.2                   # proportion of dataset to be used as cross-validation set
Nmax = 2                       # for feature at day t, we use lags from t-1, t-2, ..., t-N as features
                                # Nmax is the maximum N we are going to test
fontsize = 14
ticklabelsize = 14
####################################

#tix = input('Ticker: ')
df1 = wb.get_data_yahoo('AMZN', start = start)
df1.to_csv(r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_1011.csv')

####################################

def get_preds_mov_avg(df, target_col, N, pred_min, offset):
    """
    Given a dataframe, get prediction at timestep t using values from t-1, t-2, ..., t-N.
    Using simple moving average.
    Inputs
        df         : dataframe with the values you want to predict. Can be of any length.
        target_col : name of the column you want to predict e.g. 'adj_close'
        N          : get prediction at timestep t using values from t-1, t-2, ..., t-N
        pred_min   : all predictions should be >= pred_min
        offset     : for df we only do predictions for df[offset:]. e.g. offset can be size of training set
    Outputs
        pred_list  : list. The predictions for target_col. np.array of length len(df)-offset.
    """
    pred_list = df[target_col].rolling(window = N, min_periods=1).mean() # len(pred_list) = len(df)
    
    # Add one timestep to the predictions
    pred_list = np.concatenate((np.array([np.nan]), np.array(pred_list[:-1])))
    
    # If the values are < pred_min, set it to be pred_min
    pred_list = np.array(pred_list)
    pred_list[pred_list < pred_min] = pred_min
    
    return pred_list[offset:]

def get_mape(y_true, y_pred): 
    """
    Compute mean absolute percentage error (MAPE)
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

####################################

df = pd.read_csv(stk_path, sep = ",")
    # Convert Date column to datetime
df.loc[:, 'Date'] = pd.to_datetime(df['Date'],format='%Y-%m-%d')
    # Change all column headings to be lower case, and remove spacing
df.columns = [str(x).lower().replace(' ', '_') for x in df.columns]
    # # Get month of each sample
df['month'] = df['date'].dt.month
    # Sort by datetime
df.sort_values(by='date', inplace=True, ascending=True)

####################################

df['date'].min(), df['date'].max()
    # Plot adjusted close over time
rcParams['figure.figsize'] = 10, 8 # width 10, height 8
ax = df.plot(x='date', y='adj_close', style='b-', grid=True)
ax.set_xlabel("date")
ax.set_ylabel("USD")
ax.set_title('Plot adjusted close over time')

####################################

    # Get sizes of each of the datasets
num_cv = int(cv_size*len(df))
num_test = int(test_size*len(df))
num_train = len(df) - num_cv - num_test
print("num_train = " + str(num_train))
print("num_cv = " + str(num_cv))
print("num_test = " + str(num_test))

    # Split into train, cv, and test
train = df[:num_train].copy()
cv = df[num_train:num_train+num_cv].copy()
train_cv = df[:num_train+num_cv].copy()
test = df[num_train+num_cv:].copy()
print("train.shape = " + str(train.shape))
print("cv.shape = " + str(cv.shape))
print("train_cv.shape = " + str(train_cv.shape))
print("test.shape = " + str(test.shape))

####################################

test['date'].min(), test['date'].max()

### EDA
# Plot adjusted close over time
rcParams['figure.figsize'] = 10, 8 # width 10, height 8
matplotlib.rcParams.update({'font.size': 14})

ax = train.plot(x='date', y='adj_close', style='b-', grid=True)
ax = cv.plot(x='date', y='adj_close', style='y-', grid=True, ax=ax)
ax = test.plot(x='date', y='adj_close', style='g-', grid=True, ax=ax)
ax.legend(['train', 'validation', 'test'])
ax.set_xlabel("date")
ax.set_ylabel("USD")


df.head(10)

"""## PREDICT USING MOVING AVG¶"""

RMSE = []
mape = []
for N in range(1, Nmax+1): # N is no. of samples to use to predict the next value
    est_list = get_preds_mov_avg(train_cv, 'adj_close', N, 0, num_train)
    
    cv.loc[:, 'est' + '_N' + str(N)] = est_list
    RMSE.append(math.sqrt(mean_squared_error(est_list, cv['adj_close'])))
    mape.append(get_mape(cv['adj_close'], est_list))
print('RMSE = ' + str(RMSE))
print('MAPE = ' + str(mape))


# Plot RMSE versus N
plt.figure(figsize=(12, 8), dpi=80)
plt.plot(range(1, Nmax+1), RMSE, 'x-')
plt.grid()
plt.xlabel('N')
plt.ylabel('RMSE')


# Plot MAPE versus N. Note for MAPE smaller better. 
plt.figure(figsize=(12, 8), dpi=80)
plt.plot(range(1, Nmax+1), mape, 'x-')
plt.grid()
plt.xlabel('N')
plt.ylabel('MAPE')


# Set optimum N
N_opt = 1

df.head()

"""## PLOT PREDICTIONS ON DEV SET"""

# Plot adjusted close over time
rcParams['figure.figsize'] = 10, 8 # width 10, height 8
matplotlib.rcParams.update({'font.size': 14})

ax = train.plot(x='date', y='adj_close', style='b-', grid=True)
ax = cv.plot(x='date', y='adj_close', style='y-', grid=True, ax=ax)
ax = test.plot(x='date', y='adj_close', style='g-', grid=True, ax=ax)
ax = cv.plot(x='date', y='est_N1', style='r-', grid=True, ax=ax)
ax = cv.plot(x='date', y='est_N2', style='m-', grid=True, ax=ax)
ax.legend(['train', 'validation', 'test', 'predictions with N=1',
           'predictions with N=2'])
ax.set_xlabel("date")
ax.set_ylabel("USD")


    # Plot adjusted close over time
rcParams['figure.figsize'] = (10, 8) # width 10, height 8

ax = train.plot(x='date', y='adj_close', style='bx-', grid=True)
ax = cv.plot(x='date', y='adj_close', style='yx-', grid=True, ax=ax)
ax = test.plot(x='date', y='adj_close', style='gx-', grid=True, ax=ax)
ax = cv.plot(x='date', y='est_N1', style='rx-', grid=True, ax=ax)
ax = cv.plot(x='date', y='est_N2', style='mx-', grid=True, ax=ax)
ax.legend(['train', 'validation', 'test', 'predictions with N=1',
           'predictions with N=2'])
ax.set_xlabel("date")
ax.set_ylabel("USD")
ax.set_xlim([date(2017, 11, 1), date(2017, 12, 30)])
ax.set_ylim([1000, 1300])
ax.set_title('Zoom in to dev set')

"""## FINAL MODEL"""

est_list = get_preds_mov_avg(df, 'adj_close', N_opt, 0, num_train+num_cv)
test.loc[:, 'est' + '_N' + str(N_opt)] = est_list
print("RMSE = %0.3f" % math.sqrt(mean_squared_error(est_list, test['adj_close'])))
print("MAPE = %0.3f%%" % get_mape(test['adj_close'], est_list))


# Plot adjusted close over time
rcParams['figure.figsize'] = 10, 8 # width 10, height 8

ax = train.plot(x='date', y='adj_close', style='b-', grid=True)
ax = cv.plot(x='date', y='adj_close', style='y-', grid=True, ax=ax)
ax = test.plot(x='date', y='adj_close', style='g-', grid=True, ax=ax)
ax = test.plot(x='date', y='est_N1', style='r-', grid=True, ax=ax)
ax.legend(['train', 'validation', 'test', 'predictions with N_opt=1'])
ax.set_xlabel("date")
ax.set_ylabel("USD")
matplotlib.rcParams.update({'font.size': 14})

# Plot adjusted close over time
rcParams['figure.figsize'] = 10, 8 # width 10, height 8

ax = train.plot(x='date', y='adj_close', style='bx-', grid=True)
ax = cv.plot(x='date', y='adj_close', style='yx-', grid=True, ax=ax)
ax = test.plot(x='date', y='adj_close', style='gx-', grid=True, ax=ax)
ax = test.plot(x='date', y='est_N1', style='rx-', grid=True, ax=ax)
ax.legend(['train', 'validation', 'test', 'predictions with N_opt=1'], loc='upper left')
ax.set_xlabel("date")
ax.set_ylabel("USD")
ax.set_xlim([date(2019, 7, 30), date(2020, 2, 29)])
ax.set_ylim([1700, 2300])
ax.set_title('Zoom in to test set')

test.head()

"""## FINDINGS
* On the test set, the RMSE is 1.127 and MAPE is 0.565% using last value prediction
"""

# Save as csv
test.to_csv(r'C:/Users/gordo/PROGRAM//Data_Bank/MiscBank/1/_1011.csv')

    # Compare various methods
results_dict = {'method': ['Last Value', 'Moving Average', 'Linear Regression', 'XGBoost', 'LSTM'],
                'RMSE': [1.127, 1.27, 1.42, 1.162, 1.164],
                'MAPE(%)': [0.565, 0.64, 0.707, 0.58, 0.583]}
results = pd.DataFrame(results_dict)
results

"""# finance test"""

import pandas as pd
import numpy as np
import datetime
import matplotlib.pyplot as plt
from pandas_datareader import data as pdr
import yfinance as yf
from datetime import date, datetime, time, timedelta


start = datetime(2017, 3, 30)
end = datetime(2020, 3, 30)

stock = pdr.get_data_yahoo('AAPL', start=start,end=end)
stock.head()

    # Inspect the index 
stock.index
    # Inspect the columns
stock.columns
    # Select only the last 10 observations of `Close`
ts = stock['Close'][-10:]
    # Check the type of `ts` 
type(ts)

    # Inspect the first rows of November-December 2006
#print(stock.loc[pd.Timestamp('2017-4-30'):pd.Timestamp('2017-5-30')].head())
    # Inspect the first rows of 2007 
#print(stock.loc['2017'].head())
    # Inspect November 2006
#print(stock.iloc[22:43])
    # Inspect the 'Open' and 'Close' values at 2006-11-01 and 2006-12-01
#print(stock.iloc[[22,43], [0, 3]])



    # Sample 20 rows
sample = stock.sample(20)
    # Print `sample`
#print(sample)
    # Resample to monthly level 
monthly_stock = stock.resample('M')
    # Print `monthly_stock`
#print(monthly_stock)


    # Add a column `diff` to `stock` 
stock['diff'] = stock.Open - stock.Close
    # Delete the new `diff` column
del stock['diff']


    # Import Matplotlib's `pyplot` module as `plt`
import matplotlib.pyplot as plt
    # Plot the closing prices for `stock`
stock['Close'].plot(grid=True)
plt.title('Plot the closing prices for `stock`')
    # Show the plot
plt.show()

#### returns #############################################################
    # Assign `Adj Close` to `daily_close`
daily_close = stock[['Adj Close']]
    # Daily returns
daily_pct_c = daily_close.pct_change()
    # Replace NA values with 0
daily_pct_c.fillna(0, inplace=True)
    # Inspect daily returns
#print('daily_pct_c: '+str(daily_pct_c))
    # Daily log returns
daily_log_returns = np.log(daily_close.pct_change()+1)
    # Print daily log returns
#print('daily_log_returns: '+str(daily_log_returns))

    # Resample `stock` to business months, take last observation as value 
monthly = stock.resample('BM').apply(lambda x: x[-1])
    # Calculate the monthly percentage change
monthly.pct_change()
    # Resample `stock` to quarters, take the mean as value per quarter
quarter = stock.resample("4M").mean()
    # Calculate the quarterly percentage change

    # Daily returns
daily_pct_c = (daily_close / daily_close.shift(1) - 1)
    # Print `daily_pct_c`
#print('Daily Returns')
#print(daily_pct_c)
print('')
print('Quarterly Returns')

#### summary statistics ######################################################
    # Import matplotlib
import matplotlib.pyplot as plt
    # Plot the distribution of `daily_pct_c`
daily_pct_c.hist(bins=50)
plt.title('Plot the distribution of `daily_pct_c`')
    # Show the plot
plt.show()
    # Pull up summary statistics
print(daily_pct_c.describe())


###### Calculate the cumulative daily returns ############################
    # Calculate the cumulative daily returns
cum_daily_return = (1 + daily_pct_c).cumprod()
    # Print `cum_daily_return`
#print(cum_daily_return)

    # Import matplotlib
import matplotlib.pyplot as plt 
    # Plot the cumulative daily returns
cum_daily_return.plot(figsize=(12,8))
plt.title('Plot the cumulative daily returns')
    # Show the plot
plt.show()

print('Print the `cum_monthly_return`')
    # Resample the cumulative daily return to cumulative monthly return 
cum_monthly_return = cum_daily_return.resample("M").mean()
    # Print the `cum_monthly_return`
print(cum_monthly_return)


###### Plot a scatter matrix with the `daily_pct_change` data ###############
from pandas_datareader import data as pdr
import yfinance as yf

def get(tickers, startdate, enddate):
    def data(ticker):
        return (pdr.get_data_yahoo(ticker, start=startdate, end=enddate))
    datas = map (data, tickers)
    return(pd.concat(datas, keys=tickers, names=['Ticker', 'Date']))

tickers = ['AAPL', 'MSFT', 'IBM', 'GOOG']
all_data = get(tickers, start, end)
daily_close_px = all_data[['Adj Close']].reset_index().pivot('Date',
                                                             'Ticker',
                                                             'Adj Close')

print('PLOT the daily percentage change for `daily_close_px`')
    # Calculate the daily percentage change for `daily_close_px`
daily_pct_change = daily_close_px.pct_change()
    # Plot the distributions
daily_pct_change.hist(bins=50, sharex=True, figsize=(12,8))
    # Show the resulting plot
plt.show()

print('')
print('Plot a scatter matrix with the `daily_pct_change` data ')
    # Plot a scatter matrix with the `daily_pct_change` data 
pd.plotting.scatter_matrix(daily_pct_change, diagonal='kde',
                           alpha=0.1,figsize=(12,12))
plt.show()


print('')
print('')
print('quarter.pct_change')
quarter.pct_change()

"""## Moving Windows"""

# Isolate the adjusted closing prices 
adj_close_px = stock['Adj Close']
    # Calculate the moving average
moving_avg = adj_close_px.rolling(window=40).mean()
    # Inspect the result
moving_avg[-10:]

#### windows of rolling means ###############################################
    # Short moving window rolling mean
stock['42'] = adj_close_px.rolling(window=40).mean()
    # Long moving window rolling mean
stock['252'] = adj_close_px.rolling(window=252).mean()
    # Plot the adjusted closing price, the short and long 
        #windows of rolling means
stock[['Adj Close', '42', '252']].plot(figsize=(10, 8))
plt.title('Plot the adjusted closing price,\
the short and long windows of rolling means')
plt.show()


## Volatility Calculation ###############################################
    # Define the minumum of periods to consider 
min_periods = 75 
    # Calculate the volatility
vol = daily_pct_change.rolling(min_periods).std() * np.sqrt(min_periods) 
    # Plot the volatility
vol.plot(figsize=(10, 8))
plt.title('Plot the volatility')
    # Show the plot
plt.show()

"""## Ordinary Least-Squares Regression (OLS)"""

# Import the `api` model of `statsmodels` under alias `sm`
import statsmodels.api as sm
from pandas import tseries
from datetime import date, datetime
    
    # Isolate the adjusted closing price
all_adj_close = all_data[['Adj Close']]
    # Calculate the returns 
all_returns = np.log(all_adj_close / all_adj_close.shift(1))
    # Isolate the stock returns 
stock1_returns = all_returns.iloc[all_returns.index.get_level_values('Ticker') == 'AAPL']
stock1_returns.index = stock1_returns.index.droplevel('Ticker')
    # Isolate the MSFT returns
stock2_returns = all_returns.iloc[all_returns.index.get_level_values('Ticker') == 'MSFT']
stock2_returns.index = stock2_returns.index.droplevel('Ticker')
    # Build up a new DataFrame with AAPL and MSFT returns
return_data = pd.concat([stock1_returns, stock2_returns], axis=1)[1:]
return_data.columns = ['AAPL', 'MSFT']
    # Add a constant 
X = sm.add_constant(return_data['AAPL'])
    # Construct the model
model = sm.OLS(return_data['MSFT'],X).fit()
    # Print the summary
print(model.summary())


    ########################################################## PLOT
plt.plot(return_data['AAPL'], return_data['MSFT'], 'r.')
ax = plt.axis()
x = np.linspace(ax[0], ax[1] + 0.01)
plt.plot(x, model.params[0] + model.params[1] * x, 'b', lw=2)
plt.grid(True)
plt.axis('tight')
plt.xlabel('Apple Returns')
plt.ylabel('Microsoft returns')
plt.title('PLOT TICKERS RETURNS vs EACH OTHER ON SCATTERPLOT')

plt.show()


    ########################################################## PLOT
return_data['MSFT'].rolling(window=252).corr(return_data['AAPL']).plot()
plt.show()

"""## Building A Trading Strategy With Python"""

# Initialize the short and long windows
short_window = 40
long_window = 100
    # Initialize the `signals` DataFrame with the `signal` column
signals = pd.DataFrame(index=stock.index)
signals['signal'] = 0.0
    # Create short simple moving average over the short window
signals['short_mavg'] = stock['Close'].rolling(window=short_window,
                                              min_periods=1,
                                              center=False).mean()
    # Create long simple moving average over the long window
signals['long_mavg'] = stock['Close'].rolling(window=long_window, 
                                             min_periods=1, 
                                             center=False).mean()
    # Create signals
signals['signal'][short_window:]=np.where(signals['short_mavg'][short_window:] 
                                            >signals['long_mavg']\
                                          [short_window:],1.0, 0.0)   
    # Generate trading orders
signals['positions'] = signals['signal'].diff()

############################################################
    # Initialize the short and long windows
short_window = 40
long_window = 100
    # Initialize the `signals` DataFrame with the `signal` column
signals = pd.DataFrame(index=stock.index)
signals['signal'] = 0.0
    # Create short simple moving average over the short window
signals['short_mavg'] = stock['Close'].rolling(window=short_window,
                                              min_periods=1,
                                              center=False).mean()
    # Create long simple moving average over the long window
signals['long_mavg'] = stock['Close'].rolling(window=long_window, 
                                             min_periods=1, 
                                             center=False).mean()
    # Create signals
signals['signal'][short_window:]=np.where(signals['short_mavg'][short_window:] 
                                            >signals['long_mavg']\
                                          [short_window:],1.0, 0.0)   
    # Generate trading orders
signals['positions'] = signals['signal'].diff()


############################################################
    # Initialize the plot figure
fig = plt.figure()
    # Add a subplot and label for y-axis
ax1 = fig.add_subplot(111,  ylabel='Price in $')
    # Plot the closing price
stock['Close'].plot(ax=ax1, color='r', lw=2.)
    # Plot the short and long moving averages
signals[['short_mavg', 'long_mavg']].plot(ax=ax1, lw=2.)
    # Plot the buy signals
ax1.plot(signals.loc[signals.positions == 1.0].index, 
         signals.short_mavg[signals.positions == 1.0],
         '^', markersize=10, color='m')
    # Plot the sell signals
ax1.plot(signals.loc[signals.positions == -1.0].index, 
         signals.short_mavg[signals.positions == -1.0],
         'v', markersize=10, color='k')
ax1.set_title('Plot the sell signals')
    # Show the plot
plt.show()

"""## Backtesting A Strategy"""

# Set the initial capital
initial_capital= float(100000.0)
    # Create a DataFrame `positions`
positions = pd.DataFrame(index=signals.index).fillna(0.0)
    # Buy a 100 shares
positions['AAPL'] = 100*signals['signal']   
      # Initialize the portfolio with value owned   
portfolio = positions.multiply(stock['Adj Close'], axis=0)
    # Store the difference in shares owned 
pos_diff = positions.diff()
    # Add `holdings` to portfolio
portfolio['holdings'] = (positions.multiply(stock['Adj Close'], axis=0)).sum(axis=1)
    # Add `cash` to portfolio
portfolio['cash'] = initial_capital - (pos_diff.multiply(stock['Adj Close'], axis=0)).sum(axis=1).cumsum()   
    # Add `total` to portfolio
portfolio['total'] = portfolio['cash'] + portfolio['holdings']
    # Add `returns` to portfolio
portfolio['returns'] = portfolio['total'].pct_change()


############################################################################

import matplotlib.pyplot as plt
fig = plt.figure()
ax1 = fig.add_subplot(111, ylabel='Portfolio value in $')
    # Plot the equity curve in dollars
portfolio['total'].plot(ax=ax1, lw=2.)
    # Plot the "buy" trades against the equity curve
ax1.plot(portfolio.loc[signals.positions == 1.0].index, 
         portfolio.total[signals.positions == 1.0],
         '^', markersize=10, color='m')
    # Plot the "sell" trades against the equity curve
ax1.plot(portfolio.loc[signals.positions == -1.0].index, 
         portfolio.total[signals.positions == -1.0],
         'v', markersize=10, color='k')
ax1.set_title('Plot the "sell" trades against the equity curve')
    # Show the plot
plt.show()

"""## Evaluating Moving Average Crossover Strategy
### SHARPE RATIO:
"""

# DISPLAY AS PERCENTAGE
def display_as_percentage(val):
    return ('{:.2f} %'.format(val * 100))
    
    # Isolate the returns of your strategy
returns = portfolio['returns']
    # annualized Sharpe ratio
sharpe_ratio = np.sqrt(252) * (returns.mean() / returns.std())
    # Print the Sharpe ratio
    
print('SHARPE RATIO: '+display_as_percentage(sharpe_ratio))

"""### Maximum Drawdown"""

# Define a trailing 252 trading day window
window = 252
    # Calculate the max drawdown in the past window days for each day
rolling_max = aapl['Adj Close'].rolling(window, min_periods=1).max()
daily_drawdown = aapl['Adj Close']/rolling_max - 1.0
    # Calculate the minimum (negative) daily drawdown
max_daily_drawdown = daily_drawdown.rolling(window, min_periods=1).min()
    # Plot the results
daily_drawdown.plot()
max_daily_drawdown.plot()
plt.title('PLOT a trailing 252 trading day window')
    # Show the plot
plt.show()


########## Compound Annual Growth Rate (CAGR) #############################
    # Get the number of days in `aapl`
days = (aapl.index[-1] - aapl.index[0]).days
    # Calculate the CAGR 
cagr = ((((aapl['Adj Close'][-1]) / aapl['Adj Close'][1])) ** (365.0/days)) - 1
    # Print CAGR
print('Compound Annual Growth Rate (CAGR) = '+display_as_percentage(cagr))

"""# A Simple Time Series Analysis Of The S&P 500 Index"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
import seaborn as sb
sb.set_style('darkgrid')

start = datetime(2017, 3, 30)
end = datetime(2020, 3, 30)
#ticker = input('Ticker: ')

stock = pdr.get_data_yahoo('^GSPC', start=start,end=end)
stock.to_csv(r'C:/Users/gordo/PROGRAM/Data_Bank/MiscBank/1/_1013.csv')
        
path = os.getcwd() + r'C:/Users/gordo/PROGRAM/Data_Bank/MiscBank/1/_1013.csv'
stock_data = pd.read_csv(r'C:/Users/gordo/PROGRAM/Data_Bank/MiscBank/1/_1013.csv')

print('')
print('^GSPC')

stock_data['Date1']=stock_data['Date']
stock_data = stock_data.set_index('Date1')
stock_data.reset_index('Date1')

stock_data

##############GET STATIONARY DATA
stock_data['First Difference']=stock_data['Close']-stock_data['Close'].shift()
stock_data['First Difference'].plot(figsize=(16, 12))
print('STATIONARY DATA')

############# exponentially increasing / log transform to the original series
stock_data['Natural Log'] = stock_data['Close'].apply(lambda x: np.log(x))
plt.title('exponentially increasing / log transform to the original series')
stock_data['Natural Log'].plot(figsize=(16, 12))
print('')

########## rolling variance statistic & compare original series & logged series
stock_data['Original Variance'] = stock_data['Close'].rolling(30).mean()
stock_data['Log Variance'] = stock_data['Natural Log'].rolling(30).mean()

fig, ax = plt.subplots(2, 1, figsize=(13, 12))
stock_data['Original Variance'].plot(ax=ax[0], title='Original Variance')
stock_data['Log Variance'].plot(ax=ax[1], title='Log Variance')
fig.tight_layout()

######## variations in the data set from logged series
stock_data['Logged First Difference'] = stock_data['Natural Log'] - stock_data['Natural Log'].shift()
plt.title('variations in the data set from logged series')
stock_data['Logged First Difference'].plot(figsize=(16, 12))
print('')

### Now let's create some lag variables y(t-1), y(t-2) etc. 
    ### and examine their relationship to y(t). 
    ### We'll look at 1 and 2-day lags along with weekly and monthly lags 
    ### to look for "seasonal" effects.
    

stock_data['Lag 1'] = stock_data['Logged First Difference'].shift()
stock_data['Lag 2'] = stock_data['Logged First Difference'].shift(2)
stock_data['Lag 5'] = stock_data['Logged First Difference'].shift(5)
stock_data['Lag 30'] = stock_data['Logged First Difference'].shift(30)

sb.jointplot('Logged First Difference', 'Lag 1',stock_data,
             kind='reg', height=13)


print('Notice how tightly packed the mass is around 0. It also appears to \
be pretty evenly distributed - the marginal distributions on both axes are \
roughly normal. This seems to indicate that knowing the index value one day \
doesnt tell us much about what it will do the next day. It probably comes as \
no surprise that theres very little correlation between the change in value \
from one day to the next. Although I didnt plot them out here, the other \
lagged variables that we created above show similar results. \
There could be a relationship to other lag steps that we havent tried, \
but its impractical to test every possible lag value manually. Fortunately\
there is a class of functions that can systematically do this for us.')
print('')
print('"seasonal" effects')

from statsmodels.tsa.stattools import acf
from statsmodels.tsa.stattools import pacf

lag_correlations = acf(stock_data['Logged First Difference'].iloc[1:])
lag_partial_correlations = pacf(stock_data['Logged First Difference'].iloc[1:])



#####  The partial auto-correlation function 
fig, ax = plt.subplots(figsize=(16,12))
ax.set_title('The partial auto-correlation function')
ax.plot(lag_correlations, marker='o', linestyle='--')
print('computes the correlation at each lag step that is NOT \
already explained by previous, lower-order lag steps.')

print('The auto-correlation and partial-autocorrelation results are very \
close to each other (I only plotted the auto-correlation results above). \
What this shows is that there is no significant (> 0.2) correlation between\
the value at time t and at any time prior to t up to 40 steps behind. \
In order words, the series is a random walk. Another interesting technique \
we can try is a decomposition. This is a technique that attempts to break \
down a time series into trend, seasonal, and residual factors. Statsmodels\
comes with a decompose function out of the box.')


from statsmodels.tsa.seasonal import seasonal_decompose

decomposition = seasonal_decompose(stock_data['Natural Log'],
                                   model='additive', freq=30)
fig = plt.figure()
fig = decomposition.plot()

print('Since we dont see any real cycle in the data, the visualization is not that \
effective in this case. For data where this is a strong seasonal pattern\
though it can be very useful. The folling instance, for example, is a sample\
from the statsmodels documentation showing CO2 emissions data over time.')

print('The decomposition is much more useful in this case. There are three \
clearly distinct components to the time series - a trend line, a seasonal \
adjustment, and residual values. Each of these would need to be accounted\
for and modeled appropriately. Going back to our stock data, were already\
observed that its a random walk and that our lagged variables dont seem \
to have much impact, but we can still try fitting some ARIMA models and\
see what we get. Lets start with a simple moving average model.')

co2_data = sm.datasets.co2.load_pandas().data
co2_data.co2.interpolate(inplace=True)
result = sm.tsa.seasonal_decompose(co2_data.co2)
fig = plt.figure()
fig = result.plot()

model = sm.tsa.ARIMA(stock_data['Natural Log'].iloc[1:], order=(1, 0, 0))
results = model.fit(disp=-1)
stock_data['Forecast'] = results.fittedvalues
stock_data[['Natural Log', 'Forecast']].plot(figsize=(16, 12))

print('So at first glance it seems like this model is doing pretty well. But although it appears like the forecasts are really close (the lines are almost indistinguishable after all), remember that we used the un-differenced series! The index only fluctuates a small percentage day-to-day relative to the total absolute value. What we really want is to predict the first difference, or the day-to-day moves. We can either re-run the model using the differenced series, or add an "I" term to the ARIMA model (resulting in a (1, 1, 0) model) which should accomplish the same thing. Lets try using the differenced series.')

model = sm.tsa.ARIMA(stock_data['Logged First Difference'].iloc[1:],
                     order=(1, 0, 0))

results = model.fit(disp=-1)
stock_data['Forecast'] = results.fittedvalues
stock_data[['Logged First Difference', 'Forecast']].plot(figsize=(16, 12))

print('Its a little hard to tell, but it appears like our forecasted changes are generally much smaller than the actual changes. It might be worth taking a closer look at a subset of the data to see whats really going on.')

stock_data[['Logged First Difference',
            'Forecast']].iloc[700:1000, :].plot(figsize=(16, 12))
print('')

model = sm.tsa.ARIMA(stock_data['Logged First Difference'].iloc[1:],
                     order=(0, 0, 1))

results = model.fit(disp=-1)
stock_data['Forecast'] = results.fittedvalues
stock_data[['Logged First Difference', 'Forecast']].plot(figsize=(16, 12))
print('You can probably guess the answer...if predicting the stock market were this easy, everyone would be doing it! As I mentioned at the top, the point of this analysis wasnt to claim that you can predict the market with these techniques, but rather to demonstrate the types of the analysis one might use when breaking down time series data. Time series analysis is a very rich field with lots more technical analysis than what I went into here (much of which Im still learning). If youre interested in doing a deeper dive, I recommend these notes from a professor at Duke. A lot of what I learned about the field I picked up from reading online resources like this one. Finally, the original source code from this post is hosted on GitHub here, along with a variety of other notebooks. Feel free to check it out!')

"""# Asset_Beta_AND_Mkt_Beta"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import statsmodels.api as sm
from statsmodels import regression
from statsmodels.tsa.stattools import adfuller
import pandas as pd
from pandas_datareader import data as pdr
from pandas.util.testing import assert_frame_equal
import yfinance as yf
from patsy import dmatrices
import datetime

# DISPLAY AS PERCENTAGE
def display_as_percentage(val):
    return ('{:.4f} %'.format(val * 100))

yf.pdr_override()

# ticker = input('Ticker: ')
ticker = 'GOOGL'
df1=pdr.get_data_yahoo(ticker, start, end)
df2=pdr.get_data_yahoo("SPY", start, end)

########## GOAL = Obtain Percent Change [.pct_change()]
    #### Also, do NOT want to obtain first (0th) element because 
    ###that will result in NaN
    
return_goog=df1.Close.pct_change()[1:]
return_spy=df2.Close.pct_change()[1:]

########### PLOT Returns Versus 
    #### we will plot the returns of Google and S&P500 against each other
plt.figure(figsize=(20,10))
return_goog.plot()
return_spy.plot()
plt.ylabel('Daily Return of '+str(ticker)+' and SPY')
plt.title('Linear Regression Plot Alpha & Beta')
plt.show()


################################### ALPHA & BETA
X=return_spy.values
Y=return_goog.values

def linreg(x,y):
    x=sm.add_constant(x)
    model=regression.linear_model.OLS(y,x).fit()
    x=x[:,1]                                       # REMOVE THE CONSTANT
    return model.params[0], model.params[1]

alpha, beta = linreg(X,Y)
print('alpha: '+display_as_percentage(alpha))
print('')
print('beta: '+display_as_percentage(beta))



##################### LINEAR REGRESSION PLOT
X2 = np.linspace(X.min(), X.max(), 100)
Y_hat = X2 * beta + alpha

plt.figure(figsize=(10,7))
plt.scatter(X, Y, alpha=0.3)                         # PLOTS RAW DATA
plt.xlabel('S&P 500 Daily Return')
plt.ylabel('GOOG Daily Return')
plt.plot(X2, Y_hat, 'r', alpha=0.9)
plt.title('Linear Regression Plot Alpha & Beta')
plt.show()

"""#### Capital Asset Pricing Model or CAPM (How to Calculate Beta)
* One of the most common ways to calculate beta is using the Capital Asset Pricing Model or CAPM. - Have a look at the CAPM model:E (Ra) = Rf + Ba [ E (Rm) - Rf]
* CAPM model states that the expected return of an asset ‘E (Ra)’ is equal to the risk-free return in the market plus the difference between the expected return of the market and risk-free rate ‘[E (Rm) - Rf]’ multiplied by the asset’s beta ‘Ba’.

* If we have all the values except the asset’s beta, we can calculate the beta using:

* Ba = [E (Ra) - Rf] / [ E (Rm) - Rf]
* We can even find beta by performing the ‘regression analysis’.

* When one tries to capture a mathematical relationship between ‘x’ and ‘y’ variables, by fitting a line, polynomial or a curve through scatter plots, such that one can make a reasonably good prediction of ‘y’ given ‘x’, then the mathematical process of deriving such an equation between x and y is called the regression analysis.This equation can also arrived at by using a machine learning based regression model.

* If we try to fit a ‘line’ through this scatter plot that “best” explains the observed values of ‘y’ in terms of observed values of ‘x’, we get a simple linear regression model.

* Linear regression assumes a linear relationship between the dependent and independent variables.

* The following regression equation describes that relation:

* Yi = b0 + b1 Xi + ei

* We refer to the intercept ‘b0’ and slope coefficient ‘b1’ as the regression coefficients and ei as the random error.

* Rasset = ex-post alpha + beta of asset * Rbenchmark + eiTo understand the coefficients more intuitively, if we consider the returns for Google vs. S&P 500 index, then the slope coefficient in a regression line is called the stock’s beta, as it measures the relative amount of systematic or undiversifiable risk in Google’s returns. * If the slope of Google returns is more than 1, its returns tend to increase or decrease more than the market returns. A slope or beta of 1 would have the same level of systematic risk as that of the market on an average, and a slope or beta less than 1 implies that the returns increase or decrease by less than the change in the market returns. The intercept term is the ex-post alpha i.e. the measure of excess returns of Google as compared to market index returns. If the intercept term is negative, it means Google has underperformed S&P on a risk adjusted basis and a positive intercept means it has had excess returns on risk adjusted basis. All the points on the regression equation line, predict the ‘y’ values for the corresponding ‘x’ values. However, the optimal regression line is the one for which the sum of the squared differences (vertical distances) or the sum of squared errors or SSE between the ‘y’ values predicted by the regression equation/line and the actual ‘y’ values is minimal. Thus, the regression line minimizes the SSE. This is the reason why simple linear regression is also called as Ordinary Least Squares or OLS, and the estimated (predicted) values by the regression equation i.e. y predicted are called least squares estimates. The slope coefficient ‘b1’ of the regression line is calculated as the covariance of x and y divided by the variance of x (covxy /σ2x ), and the intercept coefficient is the line's intersection with the y axis at x = 0.

# Quantitative_Value_Strat
"""

#############################################################################
    # Import The Libraries
import pandas as pd
from bs4 import BeautifulSoup as bs
import requests
#############################################################################
    # Define The Method To Extract Fundamental Data
def get_fundamental_data(df):
    for symbol in df.index:

        url = ("http://finviz.com/quote.ashx?t=" + symbol.lower())
        soup = bs(requests.get(url).content)  # , features='html5lib')
        for m in df.columns:
            try:
                df.loc[symbol, m] = fundamental_metric(soup, m)
            except Exception as e:
                print(symbol, 'not found')
                print(e)
                break
    return df

def fundamental_metric(soup, metric):
    return soup.find(text=metric).find_next(class_='snapshot-td2').text
#############################################################################
    # Define A List Of Stocks And The Fundamental Metrics
                #1     #2     #3    #4   #5    #6    #7    #8   #9    #10
stock_list = ['AMZN','GOOG','AXL','FB','TSLA','WMT','DIS','KO','PEP','ZNGA',
              'FICO','AMD','IBM','AAPL','LPL','LK','ZM','TTD','WORK','ATVI']

metric = ['P/B',
'P/E',
'Forward P/E',
'PEG',
'Debt/Eq',
'EPS (ttm)',
'Dividend %',
'ROE',
'ROI',
'EPS Q/Q',
'Insider Own'
]

df = pd.DataFrame(index=stock_list,columns=metric)
df = get_fundamental_data(df)

print('')
print("All stocks with fundamental data")
df

# 1. Businesses which are quoted at low valuations
#P/E < 20
#P/B < 3

try:
    df = df[(df['P/E'].astype(float) < 20) & (df['P/B'].astype(float) < 3.0)]
except:
    pass
df

# 2. Businesses which have demonstrated earning power
# EPS Q/Q > 10%

try:
    df['EPS Q/Q'] = df['EPS Q/Q'].map(lambda x: x[:-1])
    df = df[df['EPS Q/Q'].astype(float) > 10]
except:
    pass
df

# 3. Businesses earning good ROE while employing little or no debt
#Debt/Eq < 1
# ROE > 10%

try:
    df['ROE'] = df['ROE'].map(lambda x: x[:-1])
    df = df[(df['Debt/Eq'].astype(float) < 2) & (df['ROE'].astype(float) > 15)]
except:
    pass
df

# 4. Management having substantial ownership in the business
# Insider own > 30%

try:
    df['Insider Own'] = df['Insider Own'].map(lambda x: x[:-1])
    df = df[df['Insider Own'].astype(float) > 30]
except:
    pass

print("Stocks after screening")
df



